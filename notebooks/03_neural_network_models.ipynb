{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0841d8e",
   "metadata": {},
   "source": [
    "# Нейросетевое решение задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744e50b",
   "metadata": {},
   "source": [
    "#### Загрузка эмбэддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71ce7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9eddbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "stop_words.update({\n",
    "    'это', 'очень', 'вообще', 'всё', 'ещё', 'просто', 'почему', \n",
    "    'которые', 'который', 'пока', 'хотя', 'вроде', 'тебе', 'твой',\n",
    "    'чтото', 'такой', 'такие', 'такое', 'какой', 'какие', 'какое',\n",
    "    'таким', 'такими', 'такому', 'каким', 'какими', 'какому',\n",
    "    'свой', 'свои', 'свое', 'своим', 'своими', 'своему'\n",
    "})\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(text, use_lemmatization=True, min_length=2):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if (token not in stop_words and \n",
    "            token.isalpha() and \n",
    "            len(token) >= min_length):\n",
    "            \n",
    "            if use_lemmatization and morph:\n",
    "                lemma = morph.parse(token)[0].normal_form\n",
    "                cleaned_tokens.append(lemma)\n",
    "            else:\n",
    "                cleaned_tokens.append(token)\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e9b151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_idx, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            tokens = text.split()\n",
    "        else:\n",
    "            tokens = text\n",
    "            \n",
    "        seq = []\n",
    "        for word in tokens:\n",
    "            if word in self.word_to_idx:\n",
    "                seq.append(self.word_to_idx[word])\n",
    "            else:\n",
    "                seq.append(1)\n",
    "        \n",
    "        if len(seq) > self.max_len:\n",
    "            seq = seq[:self.max_len]\n",
    "        else:\n",
    "            padding_length = self.max_len - len(seq)\n",
    "            seq = seq + [0] * padding_length\n",
    "\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8bfee52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingProcessor:\n",
    "    def __init__(self, model_name='word2vec-ruscorpora-300'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.emb_dim = 300\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = api.load(self.model_name)\n",
    "        except Exception as e:\n",
    "            print(f'Model loading error: {e}')\n",
    "\n",
    "    def create_embedding_matrix(self, df, text_column_name):\n",
    "        if not self.model:\n",
    "            self.load_model()\n",
    "\n",
    "        processed_data = df[text_column_name].apply(preprocess_text).tolist()\n",
    "\n",
    "        vocab = set()\n",
    "        for t in processed_data:\n",
    "            vocab.update(t)\n",
    "\n",
    "        vocab_size = len(vocab) + 2\n",
    "        word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        embedding_matrix = np.zeros((vocab_size, self.emb_dim))\n",
    "\n",
    "        for i, word in enumerate(vocab, 2):\n",
    "            word_to_idx[word] = i\n",
    "\n",
    "            try:\n",
    "                emb_vec = self.model[word]\n",
    "                embedding_matrix[i] = emb_vec\n",
    "            except KeyError:\n",
    "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(self.emb_dim))\n",
    "\n",
    "        embedding_matrix[0] = np.zeros(self.emb_dim)\n",
    "        embedding_matrix[1] = np.random.normal(scale=0.6, size=(self.emb_dim))\n",
    "        \n",
    "        return embedding_matrix, word_to_idx, vocab, processed_data\n",
    "    \n",
    "    def save_data(self, embedding_matrix, word_to_idx, file_prefix):\n",
    "        np.save(f'../data/{file_prefix}_embedding_matrix.npy', embedding_matrix)\n",
    "        with open(f'{file_prefix}_word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(word_to_idx, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load_data(self, file_prefix):\n",
    "        embedding_matrix = np.load(f'../data/{file_prefix}_embedding_matrix.npy')\n",
    "        with open(f'{file_prefix}_word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "            word_to_idx = json.load(f)\n",
    "        \n",
    "        return embedding_matrix, word_to_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a18eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_train(df, text_column_name, label_column_name, max_len=100, batch_size=32):\n",
    "    processor = EmbeddingProcessor()\n",
    "    \n",
    "    embedding_matrix, word_to_idx, vocab, processed_texts = processor.create_embedding_matrix(\n",
    "        df, text_column_name\n",
    "    )\n",
    "    \n",
    "    texts = df[text_column_name].tolist()\n",
    "    labels = df[label_column_name].tolist()\n",
    "    \n",
    "    dataset = TextDataset(texts, labels, word_to_idx, max_len)\n",
    "    \n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    processor.save_data(embedding_matrix, word_to_idx, 'toxic_comments')\n",
    "\n",
    "    return train_loader,val_loader, embedding_matrix, word_to_idx, len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d02204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, num_classes, num_layers=2, dropout=0.3):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.vocab_size, self.embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=False,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(dropout // 2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out).squeeze(-1), dim=1)\n",
    "        context_vector = torch.sum(lstm_out * attention_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        output = self.classifier(context_vector)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28c150d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), '../models/best_nn_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'  Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Acc: {val_acc:.2f}%, Best Val Acc: {best_accuracy:.2f}%')\n",
    "        print(f'  Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        print('-' * 50)\n",
    "    \n",
    "    return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c38b1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 0.6881, Train Acc: 57.80%\n",
      "  Val Acc: 65.28%, Best Val Acc: 65.28%\n",
      "  Learning Rate: 0.000280\n",
      "--------------------------------------------------\n",
      "Epoch 2/10:\n",
      "  Train Loss: 0.6479, Train Acc: 65.30%\n",
      "  Val Acc: 68.16%, Best Val Acc: 68.16%\n",
      "  Learning Rate: 0.000761\n",
      "--------------------------------------------------\n",
      "Epoch 3/10:\n",
      "  Train Loss: 0.6189, Train Acc: 69.18%\n",
      "  Val Acc: 71.87%, Best Val Acc: 71.87%\n",
      "  Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Epoch 4/10:\n",
      "  Train Loss: 0.5619, Train Acc: 74.10%\n",
      "  Val Acc: 76.14%, Best Val Acc: 76.14%\n",
      "  Learning Rate: 0.000950\n",
      "--------------------------------------------------\n",
      "Epoch 5/10:\n",
      "  Train Loss: 0.4941, Train Acc: 80.02%\n",
      "  Val Acc: 76.27%, Best Val Acc: 76.27%\n",
      "  Learning Rate: 0.000811\n",
      "--------------------------------------------------\n",
      "Epoch 6/10:\n",
      "  Train Loss: 0.4326, Train Acc: 84.24%\n",
      "  Val Acc: 74.54%, Best Val Acc: 76.27%\n",
      "  Learning Rate: 0.000611\n",
      "--------------------------------------------------\n",
      "Epoch 7/10:\n",
      "  Train Loss: 0.3970, Train Acc: 86.36%\n",
      "  Val Acc: 75.48%, Best Val Acc: 76.27%\n",
      "  Learning Rate: 0.000388\n",
      "--------------------------------------------------\n",
      "Epoch 8/10:\n",
      "  Train Loss: 0.3773, Train Acc: 87.48%\n",
      "  Val Acc: 76.62%, Best Val Acc: 76.62%\n",
      "  Learning Rate: 0.000188\n",
      "--------------------------------------------------\n",
      "Epoch 9/10:\n",
      "  Train Loss: 0.3579, Train Acc: 88.96%\n",
      "  Val Acc: 75.72%, Best Val Acc: 76.62%\n",
      "  Learning Rate: 0.000049\n",
      "--------------------------------------------------\n",
      "Epoch 10/10:\n",
      "  Train Loss: 0.3533, Train Acc: 88.87%\n",
      "  Val Acc: 75.37%, Best Val Acc: 76.62%\n",
      "  Learning Rate: 0.000000\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/labeled.csv')\n",
    "\n",
    "train_loader, val_loader, embedding_matrix, _, _ = process_data_for_train(\n",
    "    df, \n",
    "    'comment', \n",
    "    'toxic', \n",
    "    max_len=50,\n",
    ")\n",
    "\n",
    "model = TextClassifier(\n",
    "    embedding_matrix,\n",
    "    hidden_dim=256,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "train_losses, val_accuracies = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06651080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
