{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0841d8e",
   "metadata": {},
   "source": [
    "# Нейросетевое решение задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744e50b",
   "metadata": {},
   "source": [
    "#### Загрузка эмбэддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.downloader import api\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eddbfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "stop_words.update({\n",
    "    'это', 'очень', 'вообще', 'всё', 'ещё', 'просто', 'почему', \n",
    "    'которые', 'который', 'пока', 'хотя', 'вроде', 'тебе', 'твой',\n",
    "    'чтото', 'такой', 'такие', 'такое', 'какой', 'какие', 'какое',\n",
    "    'таким', 'такими', 'такому', 'каким', 'какими', 'какому',\n",
    "    'свой', 'свои', 'свое', 'своим', 'своими', 'своему'\n",
    "})\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(text, use_lemmatization=True, min_length=2):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if (token not in stop_words and \n",
    "            token.isalpha() and \n",
    "            len(token) >= min_length):\n",
    "            \n",
    "            if use_lemmatization and morph:\n",
    "                lemma = morph.parse(token)[0].normal_form\n",
    "                cleaned_tokens.append(lemma)\n",
    "            else:\n",
    "                cleaned_tokens.append(token)\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_idx, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texsts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        seq = [self.word_to_idx.get(word, 0) for word in text]\n",
    "        seq = seq[:self.max_len]\n",
    "\n",
    "        if len(seq) < self.max_len:\n",
    "            seq += [0] * self.max_len - len(seq)\n",
    "\n",
    "        return torch.LongTensor(seq), torch.tensor(label, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfee52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingProcessor:\n",
    "    def __init__(self, model_name='word2vec-ruscorpora-300'):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.emb_dim = 300\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = api.load(self.model_name)\n",
    "        except Exception as e:\n",
    "            print(f'Model loading error: {e}')\n",
    "\n",
    "    def create_embedding_matrix(self, df, text_column_name):\n",
    "        if not self.model:\n",
    "            self.load_model()\n",
    "\n",
    "        processed_data = df[text_column_name].apply(processed_data).tolist()\n",
    "\n",
    "        vocab = set()\n",
    "        for t in processed_data:\n",
    "            vocab.update(t)\n",
    "\n",
    "        vocab_size = len(vocab) + 1\n",
    "        word_to_idx = {}\n",
    "        embedding_matrix = np.zeros((vocab_size, self.emb_dim))\n",
    "\n",
    "        for i, word in enumerate(vocab, 1):\n",
    "            word_to_idx[word] = i\n",
    "\n",
    "            try:\n",
    "                emb_vec = self.model(word)\n",
    "                embedding_matrix[i] = emb_vec\n",
    "            except KeyError:\n",
    "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(self.emb_dim))\n",
    "        \n",
    "        return embedding_matrix, word_to_idx, vocab, processed_data\n",
    "    \n",
    "    def save_data(self, embedding_matrix, word_to_idx, file_prefix):\n",
    "        np.save(f'../data/{file_prefix}_embedding_matrix.npy', embedding_matrix)\n",
    "        with open(f'{file_prefix}_word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(word_to_idx, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load_data(self, file_prefix):\n",
    "        embedding_matrix = np.load(f'../data/{file_prefix}_embedding_matrix.npy')\n",
    "        with open(f'{file_prefix}_word_to_index.json', 'w', encoding='utf-8') as f:\n",
    "            word_to_idx = json.load(f)\n",
    "        \n",
    "        return embedding_matrix, word_to_idx\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_train(df, text_column_name, label_column_name, max_len=100, batch_size=32):\n",
    "    processor = EmbeddingProcessor()\n",
    "    embedding_matrix, word_to_idx, vocab, processed_data = processor.create_embedding_matrix(df, text_column_name)\n",
    "\n",
    "    labels = df[label_column_name].values\n",
    "\n",
    "    dataset = TextDataset(processed_data, labels, word_to_idx, max_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader, embedding_matrix, word_to_idx, len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, num_classes, dropout=0.3):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.vocab_size, self.embedding_dim = embedding_matrix.shape\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=False,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim*2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        f_out = hidden[-2, :, :]\n",
    "        b_out = hidden[-1, :, :]\n",
    "        hidden_combined = np.concat((f_out, b_out), dim=1)\n",
    "\n",
    "        output = self.classifier(hidden_combined)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/labeled.csv')\n",
    "\n",
    "dataloader, embedding_matrix, word_to_idx, vocab_size = process_data_for_train(\n",
    "    df, \n",
    "    'comment', \n",
    "    'toxic', \n",
    "    max_len=50, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TextClassifier(\n",
    "    embedding_matrix,\n",
    "    hidden_dim=128,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02b55b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
